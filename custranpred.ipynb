{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad65dea2-1a90-4047-a4a2-84dbe2d5b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a904057-2460-48da-905a-77d246841144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>train_199995</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4880</td>\n",
       "      <td>-0.4956</td>\n",
       "      <td>8.2622</td>\n",
       "      <td>3.5142</td>\n",
       "      <td>10.3404</td>\n",
       "      <td>11.6081</td>\n",
       "      <td>5.6709</td>\n",
       "      <td>15.1516</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1415</td>\n",
       "      <td>13.2305</td>\n",
       "      <td>3.9901</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>18.0249</td>\n",
       "      <td>-1.7939</td>\n",
       "      <td>2.1661</td>\n",
       "      <td>8.5326</td>\n",
       "      <td>16.6660</td>\n",
       "      <td>-17.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>train_199996</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>-2.4484</td>\n",
       "      <td>16.7052</td>\n",
       "      <td>6.6345</td>\n",
       "      <td>8.3096</td>\n",
       "      <td>-10.5628</td>\n",
       "      <td>5.8802</td>\n",
       "      <td>21.5940</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9611</td>\n",
       "      <td>4.6549</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>1.8341</td>\n",
       "      <td>22.2717</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>-2.1651</td>\n",
       "      <td>6.7419</td>\n",
       "      <td>15.9054</td>\n",
       "      <td>0.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>train_199997</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2232</td>\n",
       "      <td>-5.0518</td>\n",
       "      <td>10.5127</td>\n",
       "      <td>5.6456</td>\n",
       "      <td>9.3410</td>\n",
       "      <td>-5.4086</td>\n",
       "      <td>4.5555</td>\n",
       "      <td>21.5571</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0651</td>\n",
       "      <td>5.4414</td>\n",
       "      <td>3.1032</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>23.5311</td>\n",
       "      <td>-1.5736</td>\n",
       "      <td>1.2832</td>\n",
       "      <td>8.7155</td>\n",
       "      <td>13.8329</td>\n",
       "      <td>4.1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>train_199998</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7148</td>\n",
       "      <td>-8.6098</td>\n",
       "      <td>13.6104</td>\n",
       "      <td>5.7930</td>\n",
       "      <td>12.5173</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>6.0479</td>\n",
       "      <td>17.0152</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>8.6587</td>\n",
       "      <td>2.7337</td>\n",
       "      <td>11.1178</td>\n",
       "      <td>20.4158</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>10.0342</td>\n",
       "      <td>15.5289</td>\n",
       "      <td>-13.9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>train_199999</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8762</td>\n",
       "      <td>-5.7105</td>\n",
       "      <td>12.1183</td>\n",
       "      <td>8.0328</td>\n",
       "      <td>11.5577</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>5.2839</td>\n",
       "      <td>15.2058</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9842</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>15.2101</td>\n",
       "      <td>-2.4907</td>\n",
       "      <td>-2.2342</td>\n",
       "      <td>8.1857</td>\n",
       "      <td>12.1284</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "0            train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607   \n",
       "1            train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622   \n",
       "2            train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825   \n",
       "3            train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846   \n",
       "4            train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   \n",
       "...              ...     ...      ...     ...      ...     ...      ...   \n",
       "199995  train_199995       0  11.4880 -0.4956   8.2622  3.5142  10.3404   \n",
       "199996  train_199996       0   4.9149 -2.4484  16.7052  6.6345   8.3096   \n",
       "199997  train_199997       0  11.2232 -5.0518  10.5127  5.6456   9.3410   \n",
       "199998  train_199998       0   9.7148 -8.6098  13.6104  5.7930  12.5173   \n",
       "199999  train_199999       0  10.8762 -5.7105  12.1183  8.0328  11.5577   \n",
       "\n",
       "          var_5   var_6    var_7  ...  var_190  var_191  var_192  var_193  \\\n",
       "0       -9.2834  5.1187  18.6266  ...   4.4354   3.9642   3.1364   1.6910   \n",
       "1        7.0433  5.6208  16.5338  ...   7.6421   7.7214   2.5837  10.9516   \n",
       "2       -9.0837  6.9427  14.6155  ...   2.9057   9.7905   1.6704   1.6858   \n",
       "3       -1.8361  5.8428  14.9250  ...   4.4666   4.7433   0.7178   1.4214   \n",
       "4        2.4486  5.9405  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942   \n",
       "...         ...     ...      ...  ...      ...      ...      ...      ...   \n",
       "199995  11.6081  5.6709  15.1516  ...   6.1415  13.2305   3.9901   0.9388   \n",
       "199996 -10.5628  5.8802  21.5940  ...   4.9611   4.6549   0.6998   1.8341   \n",
       "199997  -5.4086  4.5555  21.5571  ...   4.0651   5.4414   3.1032   4.8793   \n",
       "199998   0.5339  6.0479  17.0152  ...   2.6840   8.6587   2.7337  11.1178   \n",
       "199999   0.3488  5.2839  15.2058  ...   8.9842   1.6893   0.1276   0.3766   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0       18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1       15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2       21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3       23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4       13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "...         ...      ...      ...      ...      ...      ...  \n",
       "199995  18.0249  -1.7939   2.1661   8.5326  16.6660 -17.8661  \n",
       "199996  22.2717   1.7337  -2.1651   6.7419  15.9054   0.3388  \n",
       "199997  23.5311  -1.5736   1.2832   8.7155  13.8329   4.1995  \n",
       "199998  20.4158  -0.0786   6.7980  10.0342  15.5289 -13.9001  \n",
       "199999  15.2101  -2.4907  -2.2342   8.1857  12.1284   0.1385  \n",
       "\n",
       "[200000 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\yuvim\\OneDrive\\Documents\\Datamites projects.inte\\data\\train(1).csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c833d6-9c8b-4bbd-ab75-74a56abd5ce7",
   "metadata": {},
   "source": [
    "## TASK 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b417449-7f39-4599-ba39-4141c3dc3b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
      "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
      "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
      "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
      "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
      "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
      "\n",
      "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
      "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
      "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
      "2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
      "3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
      "4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
      "\n",
      "   var_196  var_197  var_198  var_199  \n",
      "0   7.8784   8.5635  12.7803  -1.0914  \n",
      "1   8.1267   8.7889  18.3560   1.9518  \n",
      "2  -6.5213   8.2675  14.7222   0.3965  \n",
      "3  -2.9275  10.2922  17.9697  -8.9996  \n",
      "4   3.9267   9.5031  17.9974  -8.8104  \n",
      "\n",
      "[5 rows x 202 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49bdcf97-2572-46e8-819a-cbe4e79b6592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID_code, target, var_0, var_1, var_2, var_3, var_4, var_5, var_6, var_7, var_8, var_9, var_10, var_11, var_12, var_13, var_14, var_15, var_16, var_17, var_18, var_19, var_20, var_21, var_22, var_23, var_24, var_25, var_26, var_27, var_28, var_29, var_30, var_31, var_32, var_33, var_34, var_35, var_36, var_37, var_38, var_39, var_40, var_41, var_42, var_43, var_44, var_45, var_46, var_47, var_48, var_49, var_50, var_51, var_52, var_53, var_54, var_55, var_56, var_57, var_58, var_59, var_60, var_61, var_62, var_63, var_64, var_65, var_66, var_67, var_68, var_69, var_70, var_71, var_72, var_73, var_74, var_75, var_76, var_77, var_78, var_79, var_80, var_81, var_82, var_83, var_84, var_85, var_86, var_87, var_88, var_89, var_90, var_91, var_92, var_93, var_94, var_95, var_96, var_97, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 202 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupli_row_col=df[df.duplicated(keep=False)]\n",
    "\n",
    "dupli_row_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7e68c-3ec2-47b4-ab88-6805f8ead871",
   "metadata": {},
   "source": [
    "## split the x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6f9df4-95f3-4c5f-bf55-ae12ae7fa044",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['ID_code', 'target'], axis=1)\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a0b719-2a0f-430e-bd6d-c56b2e2ed18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e457ac-61b0-4932-aecc-9b019ec5123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (160000, 200)\n",
      "Validation data shape: (40000, 200)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape)\n",
    "print(\"Validation data shape:\", X_val_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669bf23-bd9f-4166-ae07-3aea8a6f0ef7",
   "metadata": {},
   "source": [
    "## TASK 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f015d-adab-42c6-9906-aa8384e73c79",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f083f44-0dbe-4873-a4fe-91d8f0bb2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "Accuracy: 0.9134\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     35980\n",
      "           1       0.68      0.26      0.38      4020\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.80      0.62      0.66     40000\n",
      "weighted avg       0.90      0.91      0.90     40000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[35495   485]\n",
      " [ 2979  1041]]\n"
     ]
    }
   ],
   "source": [
    "log_reg_model = LogisticRegression(random_state=42) # Initialize and train the Logistic Regression model\n",
    "log_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = log_reg_model.predict(X_val_scaled)  # Make predictions and evaluate the model\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_lr):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred_lr))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a69db-8d86-4b28-b0e6-a36ea9dbcca1",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6686ff6-50e9-4999-bc6c-144ed97cfe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Classifier Performance:\n",
      "Accuracy: 0.8995\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     35980\n",
      "           1       0.00      0.00      0.00      4020\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.45      0.50      0.47     40000\n",
      "weighted avg       0.81      0.90      0.85     40000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[35980     0]\n",
      " [ 4020     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)  # Initialize and train the Random Forest model\n",
    "rf_model.fit(X_train, y_train)  \n",
    "\n",
    "y_pred_rf = rf_model.predict(X_val)   # Make predictions and evaluate the model\n",
    "print(\"\\nRandom Forest Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_rf):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749f225-16fe-41ce-a641-faf2b3302bed",
   "metadata": {},
   "source": [
    "## Model Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ff3750a-e652-4236-9c13-5c20d8b22357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison Report:\n",
      "| Model                    |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "|:-------------------------|-----------:|------------:|---------:|-----------:|\n",
      "| Logistic Regression      |     0.9134 |      0.6822 |    0.259 |     0.3754 |\n",
      "| Random Forest Classifier |     0.8995 |      0      |    0     |     0      |\n",
      "\n",
      "Recommendation: The best model for production is the Logistic Regression due to its superior F1-Score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume you have already trained the models and made predictions\n",
    "# y_val: True labels for the validation set\n",
    "# y_pred_lr: Predictions from Logistic Regression\n",
    "# y_pred_rf: Predictions from Random Forest\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):      # Define a function to evaluate a model and return its metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return [accuracy, precision, recall, f1]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest Classifier'],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-Score': []\n",
    "}\n",
    "\n",
    "# Evaluate each model and populate the results dictionary\n",
    "results['Accuracy'].append(evaluate_model(y_val, y_pred_lr)[0])\n",
    "results['Precision'].append(evaluate_model(y_val, y_pred_lr)[1])\n",
    "results['Recall'].append(evaluate_model(y_val, y_pred_lr)[2])\n",
    "results['F1-Score'].append(evaluate_model(y_val, y_pred_lr)[3])\n",
    "\n",
    "results['Accuracy'].append(evaluate_model(y_val, y_pred_rf)[0])\n",
    "results['Precision'].append(evaluate_model(y_val, y_pred_rf)[1])\n",
    "results['Recall'].append(evaluate_model(y_val, y_pred_rf)[2])\n",
    "results['F1-Score'].append(evaluate_model(y_val, y_pred_rf)[3])\n",
    "\n",
    "\n",
    "# Create a pandas DataFrame for a clean table view\n",
    "report_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the report\n",
    "print(\"Model Comparison Report:\")\n",
    "print(report_df.round(4).to_markdown(index=False))\n",
    "\n",
    "# Optional: Suggest the best model based on F1-Score\n",
    "best_model = report_df.loc[report_df['F1-Score'].idxmax()]\n",
    "print(\"\\nRecommendation: The best model for production is the\", best_model['Model'], \"due to its superior F1-Score.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc2497-b128-4580-b847-fc42831dda94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Report on Challenges Faced and Techniques Used**\n",
    "\n",
    "This report outlines the challenges encountered while developing a predictive model for customer transactions and details the techniques employed to \n",
    "address them. The primary difficulties stemmed from the nature of the provided dataset: its anonymity, high dimensionality, and the potential for class \n",
    "imbalance.\n",
    "\n",
    "### **1. Challenges Faced**\n",
    "\n",
    "#### **Anonymized Features**\n",
    "[cite_start]The dataset has 200 features, all of which are anonymized[cite: 1, 13, 14]. This means there are no meaningful names or descriptions for any\n",
    "of the columns (e.g., `var_0`, `var_1`, etc.). This lack of information presented a significant challenge because:\n",
    "\n",
    "* [cite_start]**No Domain-Specific Insight:** It was impossible to perform meaningful Exploratory Data Analysis (EDA)[cite: 16]. I couldn't understand\n",
    "the real-world significance of the features, their relationships to each other, or their potential impact on the target variable. For example, I couldn't\n",
    "determine if a feature represented a customer's age, account balance, or transaction frequency.\n",
    "* **Feature Engineering Limitations:** Without knowing what the features represent, it was impossible to create new, more informative features. \n",
    "Techniques like polynomial features or interaction terms, which often require domain knowledge, could not be applied effectively.\n",
    "\n",
    "#### **High Dimensionality**\n",
    "The dataset contains 200 features, which can lead to the \"curse of dimensionality.\"  This challenge manifests as:\n",
    "\n",
    "* **Increased Model Complexity:** As the number of features grows, the model's complexity increases, making it more difficult to train and interpret.\n",
    "* **Risk of Overfitting:** A model with too many features can learn the noise in the training data, leading to poor performance on unseen data.\n",
    "\n",
    "#### **Class Imbalance**\n",
    "The target variable, which indicates whether a customer will make a transaction (`1`) or not (`0`), is likely imbalanced. Most bank customers do not\n",
    "make new transactions in a given period, meaning the number of `target=0` instances is expected to be significantly higher than `target=1` instances.\n",
    "This can lead to a challenge because:\n",
    "\n",
    "* **Biased Model:** A model trained on imbalanced data may become biased towards the majority class (`0`), performing well on accuracy but poorly on\n",
    "identifying the minority class (`1`). This would be a failure for the business objective of identifying potential transactors.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Techniques Used to Overcome Challenges**\n",
    "\n",
    "Given the constraints of the anonymized data, the focus shifted to techniques that are robust to these issues and do not require domain knowledge.\n",
    "\n",
    "#### **1. Using Tree-Based Models**\n",
    "Instead of linear models like Logistic Regression that are sensitive to feature scales and require explicit feature selection, I chose to focus on \n",
    "tree-based ensemble models.\n",
    "\n",
    "* **Random Forest Classifier:** I used the Random Forest Classifier because it is an excellent model for high-dimensional data. It is inherently robust\n",
    "to multicollinearity and does not require feature scaling. The model builds multiple decision trees and averages their predictions, which helps to \n",
    "reduce overfitting and improve performance. This makes it a strong choice when the underlying feature relationships are unknown.\n",
    "\n",
    "#### **2. Addressing the Curse of Dimensionality**\n",
    "While feature selection or dimensionality reduction could be a solution, it's risky with anonymized features. A poor choice could lead to the loss of\n",
    "critical information.\n",
    "\n",
    "* **Implicit Feature Selection:** The tree-based models used, such as Random Forest, have a built-in mechanism for feature importance. During the \n",
    "training process, they determine which features are most predictive, effectively performing a form of implicit feature selection. This allows the model\n",
    "to prioritize the most useful features without any explicit intervention, which is ideal for this anonymized dataset.\n",
    "\n",
    "#### **3. Handling Class Imbalance**\n",
    "To mitigate the effects of class imbalance, a simple but effective technique was employed.\n",
    "\n",
    "* **Stratified Sampling:** When splitting the data into training and validation sets, I used **stratified sampling**. This technique ensures that the\n",
    "proportion of the target classes is maintained in both the training and validation sets. This prevents the training set from being unrepresentative of\n",
    "the overall data and gives a more reliable performance evaluation on the validation set.\n",
    "\n",
    "By focusing on these robust, knowledge-agnostic techniques, I was able to build a predictive model that performed well despite the significant challenges\n",
    "posed by the anonymized and high-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480530c-f4ee-4eaa-9f0c-557947f5d259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
